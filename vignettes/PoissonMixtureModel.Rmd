---
title: "A cure model calendar-based design"
output: rmarkdown::html_vignette
bibliography: gsDesign.bib
vignette: >
  %\VignetteIndexEntry{A cure model calendar-based design}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "ragg_png",
  dpi = 96,
  fig.retina = 1,
  fig.width = 7.2916667,
  fig.asp = 0.618,
  fig.align = "center",
  out.width = "80%"
)
```

# Introduction

We present a study design for a time-to-event outcome based on a cure model (@PoissonMixture2009).
The cure model can provide a good approximation of event accumulation when risk lowers substantially over time.
In this case, it is assumed that long-term survival is of substantial interest and there is no desire to stop and do a final analysis before substantial follow-up through 3-4 years has been allowed to accrue unless there is a substantial treatment effect with an important statistically significant finding.
It is assumed further that if substantial events have not accrued in this time period, then some sacrifice in power is acceptable.
Due to this as well as substantial variability in event accrual caused by feasible differences in event rates and treatment effect, we use calendar-based timing of analyses and calendar-based spending (@LanDeMets1989).
Logistical benefits of predictable analysis timing can be substantial.

We discuss some of the potential advantages and disadvantages of the cure model and calendar-based design cases where hazard rates for events decrease substantially over time and the true underlying distributions may meaningfully deviate from what is anticipated at the time of design.

The document is sequenced as follows:

- Background on the Poisson mixture cure model.
- Explanation of scenarios considered
- Estimation of event accrual over time
- Design assumptions
- Mapping from calendar time to event fraction
- Study design
- Considerations

We do not show code, but assume the user can access the underlying vignette source to apply the methods demonstrated.
The [Shiny web interface](https://rinpharma.shinyapps.io/gsdesign/) for the **gsDesign** can be used to largely mimic what we present here.
Key code for assumptions that users may wish to change is displayed and discussed.

# The Poisson mixture model

The Poisson mixture model is a cure model that can be useful when the failure rate in a population is expected to decline substantially over time based on historical data.
It also has the property that if control group time-to-event follows a Poisson mixture distribution, then a proportional hazards assumption for treatment effect will yield another Poisson mixture distribution for the experimental group.
The model is flexible and easy to use in that the control distribution is specified with two parameters in a transparent fashion: the cure rate and one other survival rate at an arbitrarily specified time point.

The Poisson mixture model (@PoissonMixture2009) survival function as a function of time $t$ for a control group ($c$) is written initially in terms of parameters $\lambda>0$ and $\theta>0$:

$$S_c(t)=\exp(-\theta(1-\exp(-\lambda t))).$$
We note that under a proportional hazards assumption with hazard ratio $\gamma > 0$ the survival funtion for the experimental group (e) is:

$$S_e(t)=S_c(t)^\gamma=\exp(-\theta\gamma(1-\exp(-\lambda t))).$$
We note this is a Poisson mixture model with the same general form as the control survival function.
The cure rate $p$ in the control group is by definition
$$p=S_c(\infty)=e^{-\theta}\Longleftrightarrow\theta = -\log(p).$$
This implies a cure rate of $p^\gamma$ in the experimental group. The component $\exp(-\lambda t)$ defining both $S_c(t)$ and $S_e(t)$ is an exponential survival distribution. It could be replaced with an arbitrary survival distribution on $t>0$ for the mixture model. The exponential model is simple, easy to explain, and flexible enough for many situations.
The two-parameter Poisson mixture model here can be specified by the cure rate $p$ and the assumed survival rate $S_c(t_1)$ at some time $0 <t_1<\infty.$
We can solve for $\lambda$ as follows:
$$S_c(t_1)= \exp(-\theta(1-\exp(-\lambda t_1))) \Rightarrow \lambda =  -\log(1 + \log(S_c(t_1)) / \theta) / t_1$$
For ease of use, we use the parameters $p$, $t_1$, and $S_c(t_1)$ to define the Poisson mixture model below.


## Supporting functions

We create the following functions to support examples below.

- `ppm()` computes a Poisson mixture survival function or cumulative hazard function
- `hPM()` computes Poisson mixture hazard rates

The code is not displayed here, but is available in the vignette source.

```{r, echo=FALSE}
# Poisson mixture survival
# This will likely be added to a future version of gsDesign
ppm <- function(x = 0:20, cure.prob = .5, t1 = 10, s1 = .6, lower.tail = FALSE, log.p = FALSE){
## INPUT CHECKS
  if(!is.numeric(cure.prob) || length(cure.prob) > 1 || min(cure.prob <= 0) || cure.prob >= 1) stop("cure.prob in ppm() Poisson mixture model must be single real value strictly between 0 and 1")
  if(!is.numeric(t1) || length(t1) > 1 || t1 <= 0 || t1 == Inf) stop("t1 in ppm() Poisson mixture must be single numeric value > 0 and < Inf")
  if(!is.numeric(s1) || length(s1) > 1 || s1 <= cure.prob || s1 <= 0) stop("s1 in ppm() Poisson mixture must be single numeric value > 0 and < cure.prob") 
## END INPUT CHECKS
  theta <- -log(cure.prob)
  lambda <- -log(1 + log(s1) / theta) / t1
  retval <- exp(-theta * (1 - exp(-lambda * x)))
  if (lower.tail) retval <- 1 - retval
  if (log.p){if (retval > 0){retval <- log(retval)}else{retval <- Inf}}
  return(retval)
}
# Poisson mixture hazard rate
hpm <- function(x = 0:20, cure.prob = .5, t1 = 10, s1 = .6) {
## INPUT CHECKS
  if(!is.numeric(cure.prob) || length(cure.prob) > 1 || min(cure.prob <= 0) || cure.prob >= 1) stop("cure.prob in ppm() Poisson mixture model must be single real value strictly between 0 and 1")
  if(!is.numeric(t1) || length(t1) > 1 || t1 <= 0 || t1 == Inf) stop("t1 in ppm() Poisson mixture must be single numeric value > 0 and < Inf")
  if(!is.numeric(s1) || length(s1) > 1 || s1 <= cure.prob || s1 <= 0) stop("s1 in ppm() Poisson mixture must be single numeric value > 0 and < cure.prob") 
## END INPUT CHECKS
  theta <- -log(cure.prob)
  lambda <- -log(1 + log(s1) / theta) / t1
  return(theta * lambda * exp(-lambda * x))
}
```



# Example


```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(gsDesign)
library(dplyr)
library(tibble)
library(ggplot2)
library(gt)
```

## Scenario assumptions


```{r, echo=FALSE}
# Control group assumptions for three Poisson mixture cure scenarios
# Cure probabilities
cure.prob <- c(.5, .35, .7)
# Second time point for respective models
t1 <- c(18, 18, 18)
# Survival rate at 2nd time point for respective models
s1 <- c(.65, .5, .75)
time_unit <- "month"
# Hazard ratio for experimental versus control for respective models
hr <- c(.7, .75, .7)
# Total study duration
study_duration <- c(48, 48, 48)
# This code should be updated by user for their scenario
# Enrollment duration by scenario
enroll_duration <- c(12, 12, 20)
# Dropout rate (exponential failure rate per time unit) by scenario
dropout_rate <- c(.002, .0015, .001)
# Number of bins for piecewise approximation
bins <- 10
tibble(Scenario=1:3, "Cure probability" = cure.prob, "Survival at 18 months" = s1, "Hazard ratio" = hr, 
       "Enrollment duration"=enroll_duration, "Dropout rate"=dropout_rate) %>% 
  gt() %>% tab_header("Scenario assumptions for Poisson mixture model")
```
We will plan the trial assuming Scenario 1 and study the properties of all three scenarios when calendar- and event-based spending are done.
For any setting chosen, it is ideal to be able to cite published literature and other rationale for study assumptions on the intermediate- and long-term survival assumptions. Scenarios 2 and 3 have lower and higher underlying survival, respectively, than scenario 1. 
We will assume a constant enrollment rate for the duration of enrollment.
For scenarios 1 and 2, we assume 12 months of enrollment versus 18 months for scenario 3.
The following code can be easily changed to study alternate scenarios.


We use a piecewise exponential survival distribution to approximate the Poisson mixture assumption for the design.
The degree of accuracy for this approximation can be increased by increasing the number of pieces in the approximation.
The points in the following graph indicate where underlying cumulative hazard matches the piecewise exponential of the specified cure rate models by scenario.
We note for the control group in scenario 1, if we had used an exponential distribution with the same median (24 months) that the 48 month underlying survival would be 25% rather than approximately 55% as seen here.
The piecewise failure model is used to derive the sample size and targeted events over time in the trial.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
t <- seq(0, study_duration[1] + 12, (study_duration[1] + 12) / bins)
survival <- NULL
for (scenario in 1:length(cure.prob)) {
  survival <- rbind(
    survival,
    tibble(
      Scenario = scenario, Treatment = "Control", Time = t,
      Survival = ppm(
        x = t, cure.prob = cure.prob[scenario],
        t1 = t1[scenario], s1 = s1[scenario]
      )
    ),
    tibble(
      Scenario = scenario, Treatment = "Experimental", Time = t,
      Survival = ppm(
        x = t, cure.prob = cure.prob[scenario]^hr[scenario],
        t1 = t1[scenario], s1 = s1[scenario]^hr[scenario]
      )
    )
  )
}
survival <- survival %>% mutate(Scenario = as.factor(Scenario))
ggplot(survival, aes(x = Time, y = Survival, lty = Treatment, col = Scenario)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, study_duration[1] + 12, (study_duration[1] + 12) / bins)) +
  geom_line() +
  ggtitle("Poisson Mixture Model with Proportional Hazards") +
  theme(legend.position = "bottom")
```

We also evaluate the failure rate over time, which is higher in scenario 2.
This is later used in the design derivation.
Note that the piecewise intervals used to approximate changing hazard rates can be made arbitrarily small to get more precise approximations of the above.
However, given the uncertainty of the underlying assumptions, it is not clear that this provides any advantage.

```{r, echo = FALSE}
hazard <- survival %>%
  filter(Time > 0) %>%
  group_by(Scenario, Treatment) %>%
  mutate(
    cumulative_hazard = -log(Survival),
    time_lagged = lag(Time, default = 0),
    hazard_rate = (cumulative_hazard - lag(cumulative_hazard, default = 0)) /
      (Time - time_lagged),
  ) %>%
  ungroup()
hazardC1 <- tibble(time_lagged = seq(0, study_duration[1] + 12, .5)) %>%
  mutate(hazard_rate = hpm(time_lagged, cure.prob = cure.prob[1], t1 = t1[1], s1 = s1[1]))
ggplot() +
  geom_step(
    data = hazard %>% filter(Treatment == "Control", Scenario == 1),
    aes(x = time_lagged, y = hazard_rate)
  ) +
  ylab("Hazard rate") +
  xlab("Time") +
  ggtitle("Step Function Approximated Hazard Rate for Design Cure Model",
    subtitle = "Control Group"
  ) +
  geom_line(data = hazardC1, aes(x = time_lagged, y = hazard_rate), lty = 2) +
  annotate(geom = "text", x = 35, y = .02, label = "Dashed line shows actual hazard rate") +
  scale_x_continuous(breaks=seq(0,60,12)) + xlab("Month")
```

## Event Accumulation

Based on the above model, we predict how events will accumulate for the control group, experimental group under the alternate hypothesis and overall based on either the null hypothesis of no failure rate difference or the alternate hypothesis where events accrue more slowly in the experimental group.
We do this by scenario.
We use as a denominator the final planned events under the alternate hypothesis for scenario 1.

Now we compare event accrual under the null and alternate hypothesis for each scenario, with 100% representing the targeted final events under scenario 1.
The user should not have to update the code here.
For the 3 scenarios studied, event accrual is quite different, creating difference spending issues.

```{r, echo=FALSE, echo=FALSE}
# DO NOT ALTER CODE
event_accrual <- NULL
for (scenario in 1:length(cure.prob)) {
  control_accrual <- tibble(Time = 0, Events = 0, Treatment = "Control")
  experimental_accrual <- control_accrual %>% mutate(Treatment = "Experimental")
  control <- hazard %>% filter(Scenario == scenario, Treatment == "Control")
  for (T in 1:(study_duration[1] + 12)) {
    xc <- eEvents(
      lambda = control$hazard_rate, S = (control$Time - control$time_lagged)[1:(bins - 1)],
      gamma = 100 / enroll_duration[scenario], R = enroll_duration[scenario], eta = dropout_rate[scenario],
      T = T, Tfinal = study_duration[1]
    )
    control_accrual <- rbind(control_accrual, tibble(Time = xc$T, Events = xc$d, Treatment = "Control"))
    xe <- eEvents(
      lambda = control$hazard_rate * hr[scenario], S = (control$Time - control$time_lagged)[1:(bins - 1)],
      gamma = 100 / enroll_duration[scenario], R = enroll_duration[scenario], eta = dropout_rate[scenario],
      T = T, Tfinal = study_duration[1]
    )
    experimental_accrual <- rbind(experimental_accrual, tibble(Time = xe$T, Events = xe$d, Treatment = "Experimental"))
  }
  overall_accrual <- rbind(control_accrual, experimental_accrual) %>%
    group_by(Time) %>%
    summarize(Events = sum(Events), Scenario = scenario, Hypothesis = "H1")
  # Get max planned events for enrollment rates for scenario 1 under H1
  # This will be modified for the design, but relative accrual will remain the same
  if (scenario == 1) max_events_planned <- overall_accrual$Events[study_duration[1] + 1]
  overallH0_accrual <- rbind(control_accrual, control_accrual) %>%
    group_by(Time) %>%
    summarize(Events = sum(Events), Scenario = scenario, Hypothesis = "H0") %>%
    ungroup()
  # Combine for all scenarios
  event_accrual <- rbind(
    event_accrual,
    overall_accrual,
    overallH0_accrual
  )
}
event_accrual <- event_accrual %>% mutate(EF = Events / max_events_planned, Scenario = as.factor(Scenario))
ggplot(event_accrual, aes(x = Time, y = EF, color = Scenario, lty = Hypothesis)) +
  geom_line() +
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  scale_x_continuous(breaks = seq(0, study_duration[1] + 12, 12)) +
  ylab("Fraction of planned events") +
  ggtitle(
    "Fraction of planned events expected over time by scenario",
    subtitle = "Fraction based on planned final events for scenario 1"
  ) +
  theme(legend.position = "bottom")
```

# Study design

## Design assumptions

We choose calendar-based timing for analyses as well as for spending.
This is not done automatically by the `gsSurv()` function.
There are two steps in particular that we walk through here:

- How to get information fraction levels that correspond to targeted calendar analysis times to plug in for the planned design.
- Replacing information fraction levels with calendar fraction levels for $\alpha$- and $\beta$-spending.

We begin by specifying calendar times of analysis and find corresponding fractions of final planned events and calendar time under design assumptions.
We also display the impact of the calendar-based approach in preserving more $\alpha$-spending for the final analysis.

```{r, echo=FALSE}
h1s1_EF <- event_accrual %>%
  filter(Scenario == 1 & Hypothesis == "H1" & Time %in% c(12, 24, 36, 48)) %>%
  select(Time, EF) %>% mutate("Calendar fraction" = Time / study_duration[1])
# Interim analysis timing (information fraction)
timing <- h1s1_EF$EF[1:3]
timing_calendar <- h1s1_EF$Time[1:3] / study_duration[1]
```
```{r, echo=FALSE}
# Get hazard rate info for Scenario 1 control group
control <- hazard %>% filter(Scenario == 1, Treatment == "Control")
# Failure rates
lambdaC <- control$hazard_rate
# Interval durations
S <- (control$Time - control$time_lagged)[1:(bins - 1)]
# 1-sided Type I error and Type II error
alpha <- 0.025; beta <- .1
# Test type 6: asymmetric 2-sided design, non-binding futility bound
test.type <- 6
# 1-sided Type I error used for safety (for asymmetric 2-sided design)
astar <- .2
# Spending functions (sfu, sfl) and parameters (sfupar, sflpar)
# Upper spending not dissimilar from O'Brien-Fleming
sfu <- sfHSD; sfupar <- -4
# Lower spending is relatively aggressive based on Pocock-like spending
sfl <- sfLDPocock; sflpar <- NULL
# Dropout rate (exponential parameter per unit of time)
dropout_rate <- 0.002
# Experimental / control randomization ratio
ratio <- 1
```

```{r, echo=FALSE}
calendar_spend <- sfu(alpha = alpha, t=h1s1_EF$"Calendar fraction", param = sfupar)$spend
information_spend <- sfu(alpha = alpha, t=h1s1_EF$EF, param = sfupar)$spend
h1s1_EF %>% 
  mutate("Event-based" = information_spend, "Calendar-based" = calendar_spend) %>%
  gt() %>% tab_header(title="Predicted event fraction (EF) at time of planned analyses") %>% 
  fmt_number(col="EF", decimals=2) %>% fmt_number(col="Calendar fraction", decimal=2) %>%
  tab_spanner(label = "Spending time", columns = c("EF", "Calendar fraction")) %>%
  tab_spanner(label = "Cumulative alpha-spending", columns=c("Event-based", "Calendar-based")) %>%
  fmt_number(col=c("Event-based", "Calendar-based"), n_sigfig = 2)
```


## Study design

We now assume a trial is enrolled with a constant enrollment rate over `r enroll_duration` months trial duration of `r study_duration`.
As noted above, the event accumulation pattern is highly sensitive to the assumptions of the design.
Deviations from plan in accrual, the hazard ratio overall or over time as well as relatively minor deviations from the cure model assumption could substantially change the calendar time of event-based analysis timing.
Thus, calendar-based timing and spending (@LanDeMets1989) may have some appeal to make the timing of analyses more predictable.
The main risk to this would likely be under-accumulation of the final targeted events for the trial.
The targeted 4-year window may be considered clinically important as well as an important limitation for trial duration.
Using the above predicted information fractions at 6, 12, 24, 36, 48, and 60 months to plan a calendar-based design.
We use the arguments `usTime` and `lsTime` to change to calendar-based spending for the upper and lower bounds, respectively.
However, the pattern of slowing event accumulation over time after year 1 seems reasonably likely to persist.
This means that calendar-based spending is likely to give more conservative bounds since the calendar fractions are lower than the information fractions in the text overlay of the plot after the first interim: 10%, 20%, 40%, 60%, 80% and 100%, respectively.

We now use the information fractions from the text overlay to set up a calendar-based design.

```{r, echo=FALSE}
design_calendar <-
  gsSurv(
    k = length(timing) + 1,
    alpha = alpha,
    beta = beta,
    astar = astar,
    test.type = test.type,
    timing = timing, # Planned event fractions here
    hr = hr[1],
    R = enroll_duration[1],
    gamma = 1,
    T = study_duration[1],
    minfup = study_duration[1] - enroll_duration[1],
    ratio = ratio,
    sfu = sfu,
    sfupar = sfupar,
    usTime = timing_calendar, # Use calendar-based spending
    sfl = sfl,
    sflpar = sflpar,
    lambdaC = lambdaC,
    lsTime = timing_calendar, # Use calendar-based spending
    S = S
  )
design_calendar %>%
  gsBoundSummary(exclude = c("B-value", "CP", "CP H1", "PP")) %>%
  gt() %>%
  tab_header(
    title = "Calendar-Based Design",
    subtitle = "Calendar Spending"
  )
```

## Power under alternate scenarios





# Considerations

There are a few things to note for the above design:

- The futility bounds are advisory only. In particular, the late futility bounds may be ignored since the follow-up for the full time period may merit continuing the trial.
- Substantial deviations in event accumulation would not change timing of analyses from their calendar times. This should be considered for acceptability at the time of design.
- The first efficacy bound is so extreme that it essentially makes the first analysis futility only. This is likely reasonable based on the minimal follow-up at that time.
- The Z-values and hazard ratios required for a positive efficacy finding are not terribly extreme starting from the two-year follow-up analysis. This is also probably reasonable as long as the hazard ratio differences at the bounds are clinically meaningful. A more extreme finding at year 1 may likely be required for a positive finding.
- The trial may be continued after crossing an efficacy bound for further follow-up as it is unlikely that control patients doing well would cross over to experimental therapy in absence of adverse clinical outcomes. Inference at subsequent analyses using repeated p-values (@JTBook) or sequential p-values (@LiuAnderson2008) are well-specified and interpretable as adjusted p-values.

# References
