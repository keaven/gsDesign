\section{Other \texttt{gsDesign()} parameters\label{sec:othpar}}
\subsection{Setting Type I error and power}
Type I error as input to \texttt{gsDesign()} is always one-sided and is set through the parameter \texttt{alpha}.
Type II error (1-power) is set in the parameter \texttt{beta}.
A standard design modified to have Type I error of .05  and Type II error of .2 (80\% power) rather than the default of .025 Type I and .1 Type II error is produced with the command
\begin{verbatim}
x <- gsDesign(alpha=.05, beta=.2)
\end{verbatim}

\subsection{Number and timing of analyses}
The number of analyses is set in \texttt{gsDesign()} through the parameter \texttt{k>1}, which has a default of 3.
The default for timing of analyses is to have them equally-spaced, which is indicated by the default value of \texttt{timing=1}. 
This will often not be feasible or desired due to logistical or other reasons. 
The parameter \texttt{timing} can be input as a vector of length \texttt{k} or \texttt{k-1} where $0<$ \texttt{timing[1]} $<$ \texttt{timing[2]} $<\ldots <$ \texttt{timing[k]} $=1$.
It is optional to specify \texttt{timing[k]} since it is always 1.
The values in \texttt{timing} set the proportion of statistical information available for the data analyzed at each interim analysis.
The statistical information is generally proportional to the number of observations analyzed or, for survival analysis, the number of time-to-event endpoints that have been observed.
The following compares upper bounds, number of observations at each analysis, and average number of observations at the analysis where a boundary is crossed for the default design (stored in \texttt{x}) versus an alternative analyzing after 25\%and 50\% of observations (stored in \texttt{xt}) for the CAPTURE example. 
You can see that the upper bounds are more stringent when analyses are done earlier in the trial.   
\begin{verbatim}
> n.fix <- nBinomial(p1=.15, p2=.1)
> x <- gsDesign(n.fix=n.fix)
> xt <- gsDesign(n.fix=n.fix, timing=c(.25, .5))
> 2*ceiling(x$n.I/2)
[1]  656 1310 1964
> x$upper$bound
[1] 3.010739 2.546531 1.999226
> x$en
[1] 1146.391 1451.709
> 2*ceiling(xt$n.I/2)
[1]  482  964 1926
> xt$upper$bound
[1] 3.155373 2.818347 1.983563
> xt$en
[1] 1185.173 1547.649
\end{verbatim}
Comparing the designs, we see that the average sample number is lower for the default design with evenly spaced analyses compared to the design analyzing after 25\% and 50\% of observations. 
This is true both under the null hypothesis (1146 versus 1185) and the alternate hypothesis (1452 versus 1548) in spite of a lower maximum sample size (1926 versus 1964) for the latter design.   
To understand this further we look first at the probability of crossing the lower bound at each analysis for each design below.
The columns of the matrices printed correspond to the \texttt{theta} values under the null and alternate hypotheses, respectively, while rows correspond to the analyses. 
Thus, the default design has probability of 41\% of crossing the lower bound at the first interim analysis compared to 25\% for the design with first analysis at 25\% of observations. 
By examining these probabilities as well as corresponding upper boundary crossing probabilities ({\it e.g.}, \texttt{x\$upper\$prob}) we see that by moving analyses earlier without changing spending functions we have decreased the probability of crossing an interim boundary, which explains the smaller expected sample size for the default design which uses later interim analyses.
\begin{verbatim}
> x$lower$prob
          [,1]       [,2]
[1,] 0.4056598 0.01483371
[2,] 0.4290045 0.02889212
[3,] 0.1420312 0.05627417
> xt$lower$prob
          [,1]       [,2]
[1,] 0.2546094 0.01015363
[2,] 0.3839157 0.01674051
[3,] 0.3375615 0.07310586
\end{verbatim} 
\subsection{Standardized treatment effect: \texttt{delta}}
\subsubsection{Normally distributed data}
The ``usual'' formula for sample size for a fixed design is
\begin{equation}
n=\left(\frac{Z_{1-\alpha}+Z_{1-\beta}}{\delta}\right)^2.\label{eq:n.fix}
\end{equation}
This formula originates from testing the mean of a sample of normal random variables with variance 1.
The null hypothesis is that the true mean $\theta$ equals 0, while the alternate hypothesis is that $\theta=\delta$.
The distribution of the mean of $n$ observations $\bar{X}_n$ follows a normal distribution with mean $\delta$ and standard deviation $1/n$ ({\it i.e.}, N($\delta$,$1/n$)).
Assuming $\delta>0$, the standard statistic for testing this is $Z_n=\sqrt{n}\bar{X}_n\sim$N($\sqrt{n} \delta,1$) which rejects the hypothesis that the true mean is 0 if $Z_n>Z_{1-\alpha}$.
The null hypothesis is rejected with probability $\alpha$ when the null hypothesis is true (Type I error), while the probability of rejecting under the alternate hypothesis (power or one minus Type II error) is
\begin{equation}
\Phi(Z_{1-\alpha}-\sqrt{n}\delta).\label{eq:power.fix}
\end{equation}
By fixing this probability as $1-\beta$ and solving for $n$, equation (\ref{eq:n.fix}) is derived.

Assume a set of patients is evaluated at baseline for a measure of interest, then treated with a therapy and subsequently measured for a change from baseline. Assume the within subject variance for the change from baseline is 1.
Suppose $\delta=.1$.
The default group sequential design can be obtained for such a study using the call \texttt{gsDesign(delta = .1)}, yielding a planned maximum sample size of 1125.

\subsubsection{Time to event data\label{sec:TTE}}
Equations (\ref{eq:n.fix}) and (\ref{eq:power.fix}) are used as approximations for many situations where test statistics are approximated well by the normal distribution as $n$ gets large. 
A useful example of this approximation is comparing survival distributions for two groups under the assumption that the hazard rate (``instantaneous failure rate'') for the control group ($\lambda_1(t)$) and experimental group ($\lambda_2(t)$) for any time $t>0$ are proportional as expressed by
\begin{equation}
\lambda_2(t)=e^{-\gamma}\lambda_1(t).
\end{equation} 
We have used $-\gamma$ in the exponent so that a positive value of $\gamma$ indicates lower risk in the experimental treatment group. 
The value $e^{-gamma}$ will be referred to as the hazard ratio and $\gamma$ as minus the log hazard ratio.

Note that when $\gamma=0$ there is no difference in the hazard rates between treatment groups.
A common test statistic for the null hypothesis that $\gamma=0$ is the logrank test.
We will denote this by $T(d)$ where $d$ indicates the number of events observed in the combined treatment groups.
A reasonably good approximation for its distribution is 
\begin{equation}
T(d)\sim\hbox{N}(\gamma\times V(d),V(d)).
\end{equation}
For equally sized treatment groups, $V(d)$ is approximately $d/4$.
Thus, 
\begin{equation}
Z=T(d)2/\sqrt{d}\sim\hbox{N}(\sqrt{d}\gamma/2,1).
\end{equation}
For the formulation from Section \ref{sec:assumptions} we have $\theta=\gamma/2$. 
If $\gamma=\mu$ is the alternative hypothesis to the null hypothesis $\gamma=0$, then we have $\delta=\mu/2$.
In fact, Tsiatis \cite{Tsiatis}, Sellke and Siegmund \cite{SellkeSiegmund} and Slud and Wei \cite{SludWei} have all shown that group sequential theory may be applied to censored survival data using this distributional assumption; this is also discussed by Jennison and Turnbull \cite{JTBook}. 
If we assume there are $k$ analyses after $d_1<d_2<\ldots<d_k$ events and let $I_i=d_i/4$,  $i=1,2,\ldots,k$ then we may apply the canonical distribution assumptions from (\ref{CanonicalMean}) and (\ref{CanonicalCov}).

For the cancer trial example in Section \ref{sec:CAex} we assumed $e^{-\mu}=.7$ which yields $\delta= -\ln(.7)/2=.178$. 
Applying (\ref{eq:n.fix}) with $\alpha=.025$ and $\beta=.1$ and this value of $\delta$, the number of events required is calculated as 331 compared to 330 calculated previously using the Lachin and Foulkes \cite{LachinFoulkes} method. 
We may obtain the default group sequential design by specifying $\delta$ rather than the fixed design number of events as follows: \texttt{gsDesign(delta = -log(.7)/2)}. 

We also apply this distribution theory to the non-inferiority trial for a new treatment for diabetes.
We wish to rule out a hazard ratio of 1.3 for the experimental group compared to the control group under the assumption that the risk of cardiovascular events is equal in the two treatment groups.
This implies that our null hypothesis is that $\gamma=\ln(1.3)=.262$ and the alternate hypothesis is that $\gamma=0$.
Letting $\theta=(\gamma-\ln(1.3))/2$ the null hypothesis is re-framed as $\theta=0$ and the alternative as $\theta=\ln(1.3)/2$. 
The test statistic $Z=(T(d)-\ln(1.3)\times d/4)\times 2/\sqrt{d}$ is then approximately distributed N($\sqrt{d}\theta,1$).
Substituting $\delta=\ln(1.3)/2$, $\alpha=.025$ and $\beta=.1$ in (\ref{eq:n.fix}) we come up with $d=611$.
This is within 1\% of the 617 events suggested in Section \ref{sec:motivationDiab}.
